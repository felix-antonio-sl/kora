# STACK::LLM v1.0

## Stack de Desarrollo para la Era de Asistencia por LLM

*La arquitectura de referencia para construir cualquier cosa â€” desde un inventario de kiosco hasta un enjambre de agentes â€” cuando los LLMs son tus co-desarrolladores*

Febrero 2026

---

## Ãndice

0. [Axiomas Fundacionales: Por quÃ© estas tecnologÃ­as y no otras](#0-axiomas-fundacionales)
1. [Frontend: La Capa Visual](#1-frontend-la-capa-visual)
2. [Backend: La Capa LÃ³gica](#2-backend-la-capa-lÃ³gica)
3. [Datos: La Capa de Persistencia](#3-datos-la-capa-de-persistencia)
4. [Infraestructura y Operaciones](#4-infraestructura-y-operaciones)
5. [Observabilidad](#5-observabilidad)
6. [Seguridad](#6-seguridad)
7. [El Flujo de Desarrollo AI-First](#7-el-flujo-de-desarrollo-ai-first)
8. [Context Engineering: La Nueva Disciplina](#8-context-engineering-la-nueva-disciplina)
9. [Capa de Agentes (cuando el proyecto lo requiere)](#9-capa-de-agentes-cuando-el-proyecto-lo-requiere)
10. [Stack Completo: La Tabla de Referencia](#10-stack-completo-la-tabla-de-referencia)
11. [Tres Perfiles de Proyecto](#11-tres-perfiles-de-proyecto)
12. [ConexiÃ³n con el Corpus Xanpan::Agents (opcional)](#12-conexiÃ³n-con-el-corpus-xanpanagents-opcional)

---

# 0. Axiomas Fundacionales

Este documento no es un catÃ¡logo de tecnologÃ­as favoritas. Cada decisiÃ³n emerge de **seis axiomas** que reflejan cÃ³mo los LLMs realmente procesan y generan cÃ³digo. Aplican igual si estÃ¡s construyendo un sistema de inventario para un kiosco, una plataforma SaaS, o un enjambre de agentes autÃ³nomos. Porque en los tres casos, tu co-desarrollador es un modelo de lenguaje.

Entender los axiomas es mÃ¡s importante que memorizar la tabla final. Las tecnologÃ­as cambiarÃ¡n. Los axiomas sobrevivirÃ¡n.

> ğŸ§¬ **AXIOMA 1: Tipado estÃ¡tico como guardrail cognitivo**
>
> Los sistemas de tipos no son solo herramientas para desarrolladores humanos: funcionan como una capa de validaciÃ³n que intercepta errores de generaciÃ³n antes de que alcancen runtime. TypeScript y Pydantic actÃºan como compiladores de cordura para el output del LLM. Un LLM que genera cÃ³digo sin tipos es un LLM sin barandillas al borde del precipicio.

> ğŸ§¬ **AXIOMA 2: Sobre-representaciÃ³n en entrenamiento como ventaja competitiva**
>
> Las tecnologÃ­as dominantes en GitHub (Python, TypeScript, React, PostgreSQL) estÃ¡n masivamente representadas en los datos de entrenamiento de todo LLM. Esto no es una preferencia estÃ©tica: genera patrones de generaciÃ³n con menor tasa de error y mayor coherencia estructural. Elegir una tecnologÃ­a nicho porque es "tÃ©cnicamente superior" pero tiene 1/100 de la representaciÃ³n en training data es sabotear a tu co-desarrollador.

> ğŸ§¬ **AXIOMA 3: Declaratividad sobre imperatividad**
>
> Los LLMs generan cÃ³digo mÃ¡s correcto cuando describen un estado final deseado (SQL, React JSX, Tailwind, Terraform HCL) que cuando deben orquestar secuencias imperativas paso a paso. Esto explica por quÃ© React supera a jQuery para generaciÃ³n, por quÃ© Tailwind supera a CSS imperativo, y por quÃ© SQL directo supera a query builders imperativos.

> ğŸ§¬ **AXIOMA 4: Contratos explÃ­citos como lÃ­mites de alucinaciÃ³n**
>
> JSON Schema, OpenAPI, Zod, Pydantic, y tipos estrictos definen los lÃ­mites de lo que el modelo puede generar. Sin contratos, la creatividad del modelo se vuelve impredecibilidad. Cada endpoint, cada formulario, cada transformaciÃ³n de datos debe tener un contrato tipado. Es la diferencia entre pedir "parsea esto" (invitaciÃ³n a alucinar) y "valida contra este schema" (instrucciÃ³n precisa).

> ğŸ§¬ **AXIOMA 5: Resiliencia como requisito arquitectÃ³nico, no como feature**
>
> Un sistema construido con asistencia de LLM sin tests, linting, CI y validaciÃ³n automÃ¡tica no es un sistema de producciÃ³n. Es un demo. Los LLMs son no-deterministas: dos generaciones del mismo prompt pueden producir cÃ³digo diferente. Los guardrails (tipos, tests, lint, CI) convierten esa no-determinismo en un problema manejable.

> ğŸ§¬ **AXIOMA 6: SoberanÃ­a y control de costos**
>
> Self-hosting donde sea posible. Routing inteligente de modelos para controlar costos de desarrollo asistido. Dependencias mÃ­nimas de vendors con lock-in. Un stack que depende de un solo proveedor de LLM o cloud es un stack frÃ¡gil.

---

# 1. Frontend: La Capa Visual

Ya sea un dashboard de inventario de kiosco, un portal de clientes, o la interfaz del Tablero Neural de Xanpan::Agents: todo es frontend. Y es frontend que un LLM va a ayudarte a generar, mantener y evolucionar.

## 1.1 Lenguaje y Framework Core

**TypeScript + React + Next.js (App Router)**

- **TypeScript** intercepta el 70-80% de los errores de generaciÃ³n mÃ¡s comunes: propiedades faltantes, tipos incompatibles, contratos rotos. Cuando un LLM genera un componente y el compilador rechaza `<Button onClick={handler} labell="Guardar" />`, ese typo se atrapa antes de que exista. Sin TypeScript, llega a producciÃ³n.
- **React** es el framework con mayor representaciÃ³n en datos de entrenamiento (**Axioma 2**). Los LLMs han internalizado miles de patrones composicionales. Pides "un formulario de registro con validaciÃ³n" y recibes un componente funcional con hooks, estados y manejo de errores coherente. Con frameworks menos representados, recibes mÃ¡s alucinaciones estructurales.
- **Next.js App Router** unifica server components, server actions y routing. Los Server Actions son particularmente poderosos: eliminan la necesidad de APIs REST separadas para operaciones internas. Un formulario de "agregar producto al inventario" puede tener su lÃ³gica de validaciÃ³n, persistencia y redirect en el mismo archivo. Menos superficie = menos errores de generaciÃ³n = el LLM genera flujos completos con coherencia.

**Alternativa para sitios estÃ¡ticos:** Astro cuando no necesitas interactividad rica. Un catÃ¡logo de productos de lectura, un blog, una landing page.

## 1.2 Estilos y Sistema de DiseÃ±o

**Tailwind CSS + Shadcn/UI**

- **Tailwind** convierte las decisiones de diseÃ±o en tokens textuales atÃ³micos (**Axioma 3**). Cada clase es un token predecible. `className="flex items-center gap-2 text-sm font-medium text-gray-700"` â€” el LLM combina estas clases con la misma precisiÃ³n con la que combina palabras en una oraciÃ³n. No hay cascada CSS que predecir, no hay especificidad que calcular, no hay archivos .css separados que mantener en coherencia.
- **Shadcn/UI** proporciona componentes accesibles y tipados que se copian directamente al proyecto. No es dependencia npm â€” es cÃ³digo tuyo que el LLM puede leer, modificar y extender. Un `<DataTable>` de Shadcn que necesitas personalizar para tu inventario de kiosco: el LLM lo modifica directamente porque estÃ¡ en tu repo, no escondido en node_modules.

## 1.3 ValidaciÃ³n y Estado

**Zod** como capa de validaciÃ³n unificada:

- Un schema Zod definido una vez es ley en frontend Y backend. Para un inventario de kiosco: `const ProductSchema = z.object({ name: z.string().min(1), price: z.number().positive(), stock: z.number().int().nonnegative() })`. Ese schema valida el formulario, valida el Server Action, y genera el tipo TypeScript automÃ¡ticamente. El LLM no puede inventar un campo `precio` cuando el schema dice `price`.
- **TanStack Query** para gestiÃ³n de estado del servidor: caching, revalidaciÃ³n, y optimistic updates. Un LLM genera un hook de TanStack Query con mucha mÃ¡s fiabilidad que una soluciÃ³n custom de state management.

---

# 2. Backend: La Capa LÃ³gica

## 2.1 Dos lenguajes, responsabilidades divididas

No es indecisiÃ³n. Es especializaciÃ³n. Ambos lenguajes estÃ¡n masivamente representados en training data (**Axioma 2**) y el LLM genera cÃ³digo de alta calidad en ambos.

| Lenguaje | Rol | CuÃ¡ndo |
|---|---|---|
| **TypeScript (Hono / tRPC / Next.js Server Actions)** | API de producto, lÃ³gica de negocio, BFF | Cuando el frontend es Next.js y quieres types compartidos end-to-end. Para la mayorÃ­a de proyectos web: CRUD, dashboards, inventarios. |
| **Python (FastAPI)** | Capa cognitiva, procesamiento de datos, ML, integraciÃ³n con LLMs | Cuando necesitas procesamiento pesado, integraciÃ³n con ecosistema ML/AI, o tool-calling para agentes. |

**Para proyectos simples** (inventario de kiosco, portal de gestiÃ³n, SaaS bÃ¡sico): TypeScript full-stack con Next.js Server Actions es suficiente. No necesitas Python. Un solo lenguaje, tipos compartidos, zero API boilerplate.

**Para proyectos con capa cognitiva** (chatbots, agentes, procesamiento ML): Python con FastAPI para la lÃ³gica de agentes + TypeScript para el frontend. FastAPI genera automÃ¡ticamente JSON Schema desde Pydantic, que es el formato que los LLMs consumen para tool-calling. La cadena `Pydantic Model â†’ JSON Schema â†’ Tool Definition` es nativa y sin fricciÃ³n.

## 2.2 FastAPI: cuÃ¡ndo y por quÃ©

FastAPI no es obligatorio para todo proyecto. Es obligatorio cuando necesitas:

- **Tool-calling para agentes:** La cadena Pydantic â†’ JSON Schema â†’ Tool es irremplazable.
- **APIs que otros sistemas consumen:** OpenAPI automÃ¡tico desde tipos Python. DocumentaciÃ³n gratis.
- **Procesamiento de datos:** El ecosistema Python (pandas, numpy, scikit-learn) no tiene equivalente en TS.

Para un inventario de kiosco con CRUD simple, FastAPI es overkill. Server Actions de Next.js + Drizzle ORM + PostgreSQL resuelven sin salir de TypeScript.

## 2.3 DiseÃ±o de APIs y herramientas

Independientemente del lenguaje, cada endpoint y cada herramienta sigue los mismos principios:

- **Schema tipado para input Y output.** Zod en TS, Pydantic en Python. No `any`. Nunca.
- **Docstrings/JSDoc estructurados.** El LLM los lee para entender quÃ© hace cada funciÃ³n cuando genera cÃ³digo que la llama.
- **ValidaciÃ³n antes de ejecuciÃ³n.** Input validado contra schema antes de tocar la base de datos o un servicio externo.
- **Errores tipados.** No strings de error. Tipos de error que el frontend puede discriminar y manejar.

---

# 3. Datos: La Capa de Persistencia

## 3.1 Base de Datos

**PostgreSQL.** Sin excepciones para el 95% de los casos de uso.

- **Sobre-representaciÃ³n en training data** (**Axioma 2**): Los LLMs generan SQL para PostgreSQL con precisiÃ³n notable. Queries complejas, CTEs, window functions, JSON operations: todo sale bien generado al primer intento con mÃ¡s frecuencia que con cualquier otra base.
- **pgvector** integrado nativamente para bÃºsqueda vectorial cuando lo necesites. Empiezas con CRUD simple para tu inventario. Un dÃ­a quieres bÃºsqueda semÃ¡ntica ("muÃ©strame productos similares a X"). pgvector lo habilita sin migrar de base de datos. HNSW indexes para bÃºsqueda aproximada de alta velocidad.
- **Es la base de datos, no una base de datos.** Almacena datos relacionales, JSON (jsonb), vectores (pgvector), full-text search, geospatial (PostGIS). No necesitas Redis para cache simple (tienen unlogged tables), no necesitas Elasticsearch para bÃºsqueda bÃ¡sica (tienen tsvector), no necesitas una base vectorial separada (tienen pgvector hasta ~10M vectores).

**Para escalas masivas de vectores** (>10M): Qdrant, Pinecone, o Milvus como tier especializado.

## 3.2 ORM y Query Builders

- **TypeScript:** Drizzle ORM. Type-safe, cercano a SQL puro, excelente para generaciÃ³n por LLM (**Axioma 3**: declarativo). El LLM genera queries Drizzle que son casi SQL legible. Alternativa: Prisma si el equipo prioriza DX sobre cercanÃ­a a SQL.
- **Python:** SQLAlchemy 2.0 con el nuevo estilo de queries tipadas.

## 3.3 Embeddings y bÃºsqueda vectorial (cuando lo necesites)

No todo proyecto necesita embeddings. Un inventario de kiosco no los necesita al dÃ­a 1. Pero el stack debe permitir aÃ±adirlos sin reescritura:

- **AbstracciÃ³n obligatoria:** Si decides usar embeddings, una interfaz comÃºn (`EmbeddingProvider`) que permita intercambiar modelos. Almacenar siempre el `model_id` junto al vector.
- **Modelos recomendados:** OpenAI text-embedding-3-small/large como default. Cohere embed-v4 o Nomic embed como fallback open-source (soberanÃ­a, **Axioma 6**).
- **Estrategia de migraciÃ³n:** Re-indexaciÃ³n progresiva en background cuando se cambia de modelo. Dual-read durante transiciÃ³n.

## 3.4 Memoria de agentes (cuando el proyecto lo requiere)

Tres niveles que se activan progresivamente segÃºn la complejidad del proyecto:

| Nivel | DuraciÃ³n | ImplementaciÃ³n | Necesitas si... |
|---|---|---|---|
| **Working Memory** | SesiÃ³n | Ventana de contexto del LLM | Tienes cualquier interacciÃ³n con LLM |
| **Episodic Memory** | DÃ­as-semanas | PostgreSQL + bÃºsqueda por fecha/contexto | El sistema necesita recordar interacciones pasadas |
| **Semantic Memory** | Permanente | pgvector + documentos indexados | El sistema necesita conocimiento de dominio persistente |

**GestiÃ³n de ventana de contexto:** Cuando uses LLMs en tu app, implementar compresiÃ³n progresiva. Los mensajes antiguos se resumen automÃ¡ticamente cuando la ventana supera el 70% de capacidad. El resumen se genera con un modelo econÃ³mico (Haiku, Flash, Mini).

---

# 4. Infraestructura y Operaciones

## 4.1 Principio de soberanÃ­a

**Axioma 6:** Control sobre tu infraestructura. No significa "todo on-premise." Significa: entiendes dÃ³nde estÃ¡n tus datos, cuÃ¡nto pagas, y puedes migrar si el vendor cambia tÃ©rminos.

| Componente | DecisiÃ³n | JustificaciÃ³n |
|---|---|---|
| **Empaquetado** | Docker (contenedores inmutables) | Reproducibilidad. Lo que corre en dev corre en prod. |
| **Infra base** | Depende del perfil (ver Â§11) | Desde un VPS de $5/mes hasta Kubernetes multi-cloud |
| **Repositorio** | GitHub | Fuente Ãºnica de verdad. El LLM lee tu repo para generar cÃ³digo coherente. |
| **CI/CD** | GitHub Actions | Build, test, lint, deploy. AutomÃ¡tico en cada PR. |
| **CD (GitOps)** | ArgoCD (cuando necesitas K8s) | SincronizaciÃ³n declarativa continua entre GitHub y producciÃ³n. Drift detection. |
| **IaC** | Terraform / OpenTofu | Para proyectos que necesitan infra declarativa. No necesario para un VPS simple. |
| **Secrets** | SOPS o Vault | Nunca en cÃ³digo. Variables de entorno como mÃ­nimo. Vault para equipos grandes. |

## 4.2 Pipeline de CI/CD

El pipeline mÃ­nimo que todo proyecto debe tener, independientemente del tamaÃ±o:

```
Push / PR
  â†’ lint (TypeScript compiler, Ruff para Python, ESLint)
  â†’ type check (tsc --noEmit)
  â†’ tests (vitest / pytest)
  â†’ build
  â†’ deploy (automÃ¡tico a staging, manual a prod)
```

**Para proyectos con LLMs integrados**, agregar:

```
  â†’ validaciÃ³n de schemas (JSON Schema de herramientas vÃ¡lido)
  â†’ evals de regresiÃ³n (agente contra dataset de test)
  â†’ estimaciÃ³n de costo (tokens por suite antes de ejecutar contra modelos de producciÃ³n)
```

## 4.3 Feature flags

**LaunchDarkly, Unleash, o Flagsmith.** Para proyectos simples, un JSON en la DB o una tabla `feature_flags` basta. Para producciÃ³n seria: un servicio de feature flags es la red de seguridad que permite deploy continuo sin miedo. Activas la feature para el 5% de usuarios, verificas que funciona, expandes.

---

# 5. Observabilidad

## 5.1 Stack base

| Capa | QuÃ© observa | Herramientas | Todo proyecto lo necesita? |
|---|---|---|---|
| **Infraestructura** | Server health, latencia, errores | OpenTelemetry + Prometheus + Grafana | SÃ­ (al menos uptime monitoring) |
| **AplicaciÃ³n** | Errores, performance, user flows | Sentry (errores), OpenTelemetry (traces) | SÃ­ |
| **LLM/Agente** | Costos, latencia, calidad de generaciÃ³n | Langfuse (self-hosted) | Solo si usas LLMs en la app |
| **Negocio** | MÃ©tricas de producto, conversiÃ³n | PostHog / Plausible | Recomendado |

## 5.2 Alerting

Alertas bÃ¡sicas para todo proyecto: uptime, errores 5xx, latencia p95. Para proyectos con LLMs, agregar:

- DegradaciÃ³n de calidad de respuestas (hallucination rate).
- Incremento sÃºbito en costo por sesiÃ³n (loops infinitos del agente).
- CaÃ­da en tool selection accuracy.
- Rate limiting de proveedores cercano al lÃ­mite.

---

# 6. Seguridad

## 6.1 Baseline para todo proyecto

- **HTTPS siempre.** Sin excepciones.
- **AutenticaciÃ³n:** Auth.js (NextAuth) para proyectos TS full-stack. Para APIs: JWT o API keys con rotaciÃ³n.
- **AutorizaciÃ³n:** Row-level security en PostgreSQL cuando los datos son multi-tenant. El kiosco ve solo su inventario.
- **Input validation:** Zod/Pydantic en cada boundary. Nunca confÃ­es en datos del cliente.
- **Secrets:** Nunca en cÃ³digo. Variables de entorno como mÃ­nimo.
- **Dependencies:** Dependabot o Renovate para actualizaciones automÃ¡ticas de dependencias con vulnerabilidades conocidas.

## 6.2 Seguridad adicional para LLMs

Cuando tu aplicaciÃ³n integra LLMs:

| Amenaza | Control | ImplementaciÃ³n |
|---|---|---|
| **Prompt Injection** | SeparaciÃ³n system/user | Nunca concatenar user input en system prompts. Templates con placeholders tipados. |
| **Agent-to-agent injection** | SanitizaciÃ³n inter-agente | Cuando agentes pasan datos entre sÃ­, tratar el output de un agente como untrusted input para el siguiente. Validar contra schema en cada interfaz interna. |
| **Output inseguro** | Todo output LLM = untrusted | Validar contra schema antes de ejecutar cualquier acciÃ³n derivada. |
| **Data leakage** | ClasificaciÃ³n de datos | No enviar PII a LLMs externos si no es necesario. Scrubbing automÃ¡tico. |
| **Excessive agency** | Allowlists de herramientas | Si el LLM puede llamar herramientas: cada una en allowlist explÃ­cito. |
| **Costo descontrolado** | Budget enforcement | LÃ­mites por sesiÃ³n/usuario. Circuit breaker si se excede. |

## 6.3 Aislamiento de ejecuciÃ³n (para agentes)

Cuando los agentes ejecutan herramientas que tocan el sistema operativo o servicios externos:

| Nivel | Runtime | Permisos | Caso de uso |
|---|---|---|---|
| **Nivel 1 (Read)** | Container read-only | Filesystem RO, red restringida | Queries, lectura de APIs |
| **Nivel 2 (Write)** | Container efÃ­mero | Destruido post-ejecuciÃ³n | Escritura a DB, generaciÃ³n de archivos |
| **Nivel 3 (Shell)** | MicroVM (Firecracker) | Sin red externa, timeout estricto | EjecuciÃ³n de cÃ³digo arbitrario, tests |

---

# 7. El Flujo de Desarrollo AI-First

## 7.1 CLIs de desarrollo

En 2026, tres herramientas han convergido como los co-desarrolladores de lÃ­nea de comandos mÃ¡s efectivos:

| Herramienta | Fortaleza | CuÃ¡ndo usarla |
|---|---|---|
| **Claude Code** | Refactorizaciones de contexto amplio, razonamiento multi-archivo | Cambios arquitectÃ³nicos, migraciones, features complejas que tocan mÃºltiples archivos |
| **Gemini CLI** | Contexto masivo (1M+ tokens), anÃ¡lisis multicapa | RevisiÃ³n de cÃ³digo completa, documentaciÃ³n de sistemas existentes, anÃ¡lisis de logs |
| **Codex CLI** | IteraciÃ³n rÃ¡pida en terminal, acceso a OS | Scripts, one-liners, operaciones de sistema, automatizaciÃ³n rÃ¡pida |

No son mutuamente excluyentes. Son herramientas con fortalezas diferentes. Usa la que mejor se adapte a la tarea.

## 7.2 Principios del desarrollo AI-first

Desarrollar con asistencia de LLM no es "autocompletado glorificado." Es un paradigma diferente:

- **Type-first development:** Define tipos e interfaces ANTES de implementar. El LLM genera implementaciones mÃ¡s correctas cuando tiene el contrato completo. Escribir `interface Product { id: string; name: string; price: number; stock: number }` antes de pedir "implementa el CRUD de productos" produce cÃ³digo dramÃ¡ticamente mejor que pedir el CRUD sin el tipo.
- **Small PRs:** Los LLMs generan mejor cÃ³digo en cambios pequeÃ±os y enfocados que en refactorizaciones masivas. "Agrega validaciÃ³n de stock negativo al formulario de producto" > "Refactoriza todo el mÃ³dulo de inventario."
- **Context engineering como prÃ¡ctica diaria:** Mantener archivos de contexto actualizados que el LLM consume en cada sesiÃ³n. Ver Â§8.
- **Eval-driven (para agentes):** Cuando construyes agentes, escribir los evals antes de la implementaciÃ³n. Los evals definen el comportamiento esperado. Es TDD para el mundo de agentes.
- **Review todo lo generado:** El LLM es un co-desarrollador junior con conocimiento enciclopÃ©dico pero sin juicio. Revisa cada PR como si viniera de un junior brillante pero propenso a errores sutiles.

---

# 8. Context Engineering: La Nueva Disciplina

## 8.1 Â¿QuÃ© es?

Context engineering es el diseÃ±o, creaciÃ³n y mantenimiento de los artefactos que alimentan la ventana de contexto del LLM para que produzca outputs correctos. Es el equivalente a la documentaciÃ³n tÃ©cnica, pero escrita para ser consumida por mÃ¡quinas ademÃ¡s de humanos.

**El contexto es el multiplicador.** Un LLM de primera lÃ­nea con context engineering pobre produce peor resultado que un LLM de segunda lÃ­nea con context engineering excelente. Si le pides a Claude Opus que genere un endpoint sin decirle tus convenciones, patrones, o estructura del proyecto, obtienes cÃ³digo genÃ©rico. Si le das CONVENTIONS.md + ARCHITECTURE.md + el schema de la DB, obtienes cÃ³digo que encaja en tu proyecto como si lo hubiera escrito alguien del equipo.

## 8.2 Artefactos de context engineering

La inversiÃ³n en estos archivos se paga sola en la primera semana de desarrollo asistido:

| Artefacto | Contenido | Ejemplo para inventario de kiosco |
|---|---|---|
| **CONVENTIONS.md** | Estilo de cÃ³digo, patrones, naming, estructura de archivos | "Usamos camelCase en TS, snake_case en Python. Server Actions en `app/actions/`. Componentes en `components/ui/`." |
| **ARCHITECTURE.md** | Diagrama de componentes, flujo de datos, decisiones clave | "Next.js full-stack. PostgreSQL en Supabase. Auth con Auth.js. Deploy en Vercel." |
| **STACK.md** | TecnologÃ­as, versiones, quirks conocidos | "Next.js 15.1, Drizzle 0.38, PostgreSQL 16. Nota: Drizzle no soporta `returning()` en SQLite." |
| **SCHEMA.md** | Modelo de datos con relaciones | "Productos â†’ CategorÃ­as (N:1). Movimientos de inventario con timestamp y usuario." |

**Para proyectos mÃ¡s complejos**, agregar:

| Artefacto | CuÃ¡ndo lo necesitas |
|---|---|
| **INFRA.md** | Cuando tienes infra propia (no solo PaaS) |
| **CONSTRAINTS.md** | Cuando hay restricciones de compliance, budget, o regulatorias |
| **RUNBOOKS.md** | Cuando operas producciÃ³n y necesitas procedimientos de recuperaciÃ³n |
| **AGENTS.md** | Cuando tu proyecto incluye agentes IA con roles y permisos |

## 8.3 La economÃ­a del contexto

La ventana de contexto tiene un precio literal (tokens) y un precio cognitivo (diluciÃ³n de atenciÃ³n del modelo):

- **Regla 70/30:** El 70% de la ventana debe ser relevante para la tarea actual. El 30% restante es contexto de sistema. Si el ratio se invierte, el output se degrada.
- **Carga selectiva:** No cargar todo en cada sesiÃ³n. Un cambio de estilos necesita CONVENTIONS.md y los componentes afectados. No necesita INFRA.md.
- **Densidad:** Los context files deben ser densos y sin redundancia. Cada palabra es un token que se paga. "Usamos Drizzle ORM con PostgreSQL. Migraciones en `drizzle/migrations/`. Schema en `src/db/schema.ts`." â€” 20 tokens que ahorran 200 tokens de explicaciÃ³n en cada sesiÃ³n.

---

# 9. Capa de Agentes (cuando el proyecto lo requiere)

**No todo proyecto necesita agentes.** Un inventario de kiosco no necesita un enjambre auto-evolutivo. Pero cuando tu proyecto sÃ­ integra LLMs como parte de la funcionalidad (chatbots, asistentes, procesamiento inteligente, agentes autÃ³nomos), esta capa se activa.

## 9.1 Model Router

No todas las tareas requieren el mismo modelo. Un resumen de texto no necesita Opus; Haiku lo resuelve. Una decisiÃ³n arquitectÃ³nica compleja sÃ­ necesita Opus.

| Tier | Modelos (febrero 2026) | Costo relativo | Caso de uso |
|---|---|---|---|
| **T1 (EconÃ³mico)** | Haiku 3.5, GPT-4o Mini, Flash 2.0, DeepSeek-V3 | $ | ClasificaciÃ³n, formateo, resÃºmenes, orquestaciÃ³n |
| **T2 (Balance)** | Sonnet 4, GPT-4.1, Gemini 2.5 Pro | $$ | GeneraciÃ³n de cÃ³digo, anÃ¡lisis, tool-calling |
| **T3 (Frontier)** | Opus 4.5, GPT-4.5, Gemini Ultra | $$$ | Razonamiento complejo, planificaciÃ³n, arquitectura |
| **T4 (Reasoning)** | o3, Gemini Thinking | $$$$ | Problemas matemÃ¡ticos, lÃ³gicos, evaluaciÃ³n crÃ­tica |

**ImplementaciÃ³n prÃ¡ctica:** LiteLLM proxy como 80% de la soluciÃ³n. Interfaz OpenAI-compatible para todos los providers, fallback chains, budget tracking. Para el 100%: router custom con clasificador de complejidad.

**Budget enforcement:** LÃ­mites por sesiÃ³n, por usuario, por agente. Cuando se alcanza el lÃ­mite, degrada al tier inferior con notificaciÃ³n. Sin esto, un loop infinito de un agente te puede costar cientos de dÃ³lares en minutos.

## 9.2 OrquestaciÃ³n de agentes

El ecosistema cambia cada 3 meses. La respuesta correcta: **capa de abstracciÃ³n.**

Define una interfaz `AgentOrchestrator` con mÃ©todos estÃ¡ndar (`route`, `execute_tool`, `manage_memory`). Implementa sobre el framework del momento. Si el framework cambia, reescribes el adaptador, no los agentes ni las herramientas.

| Framework | Fortaleza | Caso de uso ideal |
|---|---|---|
| **OpenClaw** | Multi-agente nativo, multi-canal, persistencia local | Enjambres de agentes colaborativos |
| **LangGraph** | Flujos complejos con estado, visualizaciÃ³n | Workflows multi-paso con branching |
| **Agents SDK (OpenAI)** | API limpia, handoffs nativos | Equipos centrados en GPT |
| **Custom** | Control total | Necesidades Ãºnicas |

## 9.3 Model Context Protocol (MCP)

MCP es el estÃ¡ndar para interoperabilidad de herramientas entre agentes. Las herramientas expuestas como MCP servers son consumibles desde Claude Code, Cursor, IDEs, y tu orquestador sin reescritura. Es HTTP para el mundo de agentes: infraestructura, no feature.

## 9.4 Evals: el TDD de los agentes

| Tipo | QuÃ© verifica | Obligatorio? |
|---|---|---|
| **RegresiÃ³n** | Calidad se mantiene ante cambios | SÃ­, pre-deploy |
| **AlucinaciÃ³n** | Output no contiene informaciÃ³n fabricada | SÃ­, pre-deploy |
| **Tool-calling** | Selecciona herramientas correctas | SÃ­, pre-deploy |
| **Costo** | Tokens en rango esperado | SÃ­, pre-deploy |
| **Seguridad** | No expone datos, no escala privilegios | SÃ­, pre-deploy |
| **Adversarial** | Resiste intentos de romperlo | Recomendado, cada ciclo |

**LLM-as-a-Judge:** Un modelo de otro provider evalÃºa la calidad. Si tu agente usa Claude, el judge usa GPT. Diversidad de modelos reduce blind spots compartidos.

## 9.5 Modelos fundacionales

El sistema es agnÃ³stico por diseÃ±o. Ninguna decisiÃ³n asume un proveedor:

- **Claude (Anthropic):** Razonamiento, tool-calling preciso, instrucciones complejas.
- **Gemini (Google):** Contexto masivo (1M+), multimodalidad, eficiencia de costo.
- **GPT (OpenAI):** Ecosistema maduro, function calling robusto, reasoning (o-series).
- **Eficiencia (DeepSeek, Qwen):** Costo/rendimiento superior para T1. Self-hosteable.

> âš¡ **MODELS.md COMO ARTEFACTO VIVO**
>
> Las capacidades, precios, y disponibilidad de modelos cambian semanalmente. Esta lista es orientativa al momento de escritura (febrero 2026). La recomendaciÃ³n operativa es mantener un archivo `MODELS.md` en el repositorio del proyecto como artefacto de context engineering: modelos permitidos, tiers asignados, precios actuales, benchmarks internos. Este archivo se actualiza mensualmente (o ante cada cambio significativo de provider) y alimenta al Model Router. Tratar la informaciÃ³n de modelos como dato estÃ¡tico en un documento es un anti-patrÃ³n; tratarla como artefacto versionado y actualizable es context engineering.

## 9.6 Modos de fallo del stack (prevenciÃ³n)

Todo stack tiene modos de fallo especÃ­ficos. Estos son los mÃ¡s probables cuando LLMs participan en el desarrollo y operaciÃ³n:

| Modo de fallo | DescripciÃ³n | PrevenciÃ³n |
|---|---|---|
| **Model version drift** | Un provider actualiza silenciosamente el modelo y el comportamiento cambia. Tests pasan pero el output degrada en calidad, estilo, o precisiÃ³n. | Pinning de versiones de modelo cuando el provider lo permita. Evals de regresiÃ³n ante cada cambio de modelo. |
| **Embedding drift** | Los embeddings generados con una versiÃ³n de modelo no son compatibles con los generados con otra. La bÃºsqueda semÃ¡ntica degrada sin error visible. | Regenerar embeddings completos ante cambios de modelo de embedding. Monitorear hit rate de bÃºsqueda semÃ¡ntica. |
| **CorrupciÃ³n de memoria semÃ¡ntica** | Si usas memoria persistente para agentes (Â§3.4), datos incorrectos entran en la memoria y se amplifican con el uso. | ValidaciÃ³n de escritura en memoria. TTL para memorias. Mecanismo de purge manual. AuditorÃ­a periÃ³dica. |
| **Cost explosion por loop** | Un agente entra en un loop de reintentos, consumiendo tokens sin converger. Prompt injection puede provocar esto deliberadamente. | Budget enforcement por sesiÃ³n y por agente. Circuit breaker: si tokens consumidos > NÃ—esperado, abort. |
| **Context window overflow** | El contexto acumulado excede la ventana y el LLM empieza a "olvidar" instrucciones tempranas, generando output inconsistente. | Monitoreo de uso de contexto. Summarization estratÃ©gica. Regla 70/30 de Â§8.3. |
| **Provider downtime cascade** | Tu provider principal cae y no tienes fallback configurado. | LiteLLM con fallback a segundo provider. Nunca depender de un solo provider para funcionalidad crÃ­tica en producciÃ³n. |

---

# 10. Stack Completo: La Tabla de Referencia

## 10.1 Stack base (todo proyecto)

| Capa | TecnologÃ­a | Alternativa | Axioma |
|---|---|---|---|
| **Lenguaje frontend** | TypeScript | â€” | 1, 2 |
| **Framework frontend** | Next.js (App Router) | Astro (estÃ¡ticos) | 2, 3 |
| **Estilos** | Tailwind CSS | â€” | 3 |
| **Componentes UI** | Shadcn/UI | Radix primitives | 2 |
| **ValidaciÃ³n** | Zod (TS) / Pydantic (Py) | â€” | 1, 4 |
| **Backend (TS)** | Next.js Server Actions / Hono | tRPC | 3 |
| **Backend (Python)** | FastAPI | â€” | 4 |
| **Base de datos** | PostgreSQL | â€” | 2 |
| **ORM (TS)** | Drizzle | Prisma | 3 |
| **ORM (Python)** | SQLAlchemy 2.0 | â€” | 2 |
| **Empaquetado** | Docker | â€” | 5 |
| **Repositorio** | GitHub | GitLab | 2 |
| **CI/CD** | GitHub Actions | GitLab CI | 5 |
| **Observabilidad** | OpenTelemetry + Sentry | Datadog | 6 |
| **Auth** | Auth.js (NextAuth) | Clerk, Supabase Auth | 6 |

## 10.2 Stack extendido (proyectos con LLMs/agentes)

| Capa | TecnologÃ­a | Alternativa | Axioma |
|---|---|---|---|
| **Vectores** | pgvector (< 10M) | Qdrant (>10M) | 2 |
| **Embeddings** | OpenAI text-embedding-3 | Cohere, Nomic | 6 |
| **Observabilidad LLM** | Langfuse (self-hosted) | LangSmith | 6 |
| **Evals** | Braintrust / Arize Phoenix | TruLens | 4, 5 |
| **Model Router** | LiteLLM proxy / custom | â€” | 5, 6 |
| **OrquestaciÃ³n** | OpenClaw (tras abstracciÃ³n) | LangGraph, Agents SDK | 5 |
| **Interop** | MCP | â€” | 5 |
| **Aislamiento** | gVisor / Firecracker | Docker rootless | 5 |
| **GitOps** | ArgoCD | Flux | 5 |
| **Feature flags** | Unleash / Flagsmith | LaunchDarkly | 6 |
| **Dev tools** | Claude Code + Gemini CLI + Codex CLI | Cursor, Windsurf | 2 |

---

# 11. Tres Perfiles de Proyecto

## 11.1 Perfil MÃ­nimo: "El Kiosco"

Un inventario de kiosco. Un SaaS de agenda. Un portal de gestiÃ³n interna. CRUD con algo de lÃ³gica de negocio. Sin agentes.

| DecisiÃ³n | ElecciÃ³n |
|---|---|
| **Stack** | TypeScript full-stack. Next.js + Server Actions + Drizzle + PostgreSQL |
| **Infra** | Vercel (frontend) + Supabase o Neon (PostgreSQL). O un VPS de $5-10/mes con Docker Compose. |
| **CI/CD** | GitHub Actions: lint + type check + test + deploy |
| **Observabilidad** | Sentry (errores) + Vercel Analytics o Plausible |
| **Context engineering** | CONVENTIONS.md + SCHEMA.md. Dos archivos. |
| **Desarrollo** | Un humano + Claude Code / Cursor. |
| **Costo operativo mensual** | $5-50 |

## 11.2 Perfil Medio: "El SaaS con IA"

Una plataforma con usuarios, pagos, y una capa de IA: chatbot de soporte, anÃ¡lisis inteligente de datos, generaciÃ³n de contenido. LLMs integrados pero no agentes autÃ³nomos.

| DecisiÃ³n | ElecciÃ³n |
|---|---|
| **Stack** | TypeScript (Next.js) + Python (FastAPI para capa IA). PostgreSQL + pgvector. |
| **Infra** | Docker en VPS (Hetzner) o cloud manejado. ArgoCD si Kubernetes. |
| **CI/CD** | GitHub Actions: lint + types + tests + evals de IA + deploy |
| **Model Router** | LiteLLM proxy con budget enforcement |
| **Observabilidad** | Sentry + Langfuse + Prometheus + Grafana |
| **Context engineering** | CONVENTIONS.md + ARCHITECTURE.md + STACK.md + SCHEMA.md |
| **Desarrollo** | 1-3 humanos + LLMs como co-developers |
| **Costo operativo mensual** | $50-500 |

## 11.3 Perfil Completo: "El Enjambre"

Un sistema con agentes autÃ³nomos que ejecutan tareas, toman decisiones, y se auto-optimizan. El escenario de Xanpan::Agents.

| DecisiÃ³n | ElecciÃ³n |
|---|---|
| **Stack** | Stack completo Â§10.1 + Â§10.2. Dual language. Model Router custom. |
| **Infra** | Kubernetes con ArgoCD. Firecracker para aislamiento de agentes. IaC con Terraform. |
| **CI/CD** | Pipeline completo con evals (regresiÃ³n, alucinaciÃ³n, tool-calling, seguridad, adversarial) |
| **Model Router** | Custom con clasificador de complejidad + circuit breakers + budget enforcement |
| **Observabilidad** | Stack completo: OTEL + Prometheus + Grafana + Langfuse + alerting AI-native |
| **Context engineering** | Suite completa Â§8.2: CONVENTIONS, ARCHITECTURE, STACK, SCHEMA, INFRA, CONSTRAINTS, RUNBOOKS, AGENTS |
| **Desarrollo** | PO + Operador + enjambre (Xanpan::Agents) |
| **Costo operativo mensual** | $500-5000+ |

---

# 12. ConexiÃ³n con el Corpus Xanpan::Agents (opcional)

Este documento funciona de forma autÃ³noma. Es un stack de referencia para cualquier proyecto construido con asistencia de LLM. Pero cuando se usa en conjunto con el corpus Xanpan::Agents, forma parte de una trinidad:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        CHAPTER 0: El Operador            â”‚
â”‚        Solitario                         â”‚
â”‚   Bootstrap: DÃ“NDE empezar              â”‚
â”‚   (Punto de entrada al corpus)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          STACK::LLM v1.0                 â”‚ â† Este documento
â”‚   Arquitectura: CON QUÃ‰ construir       â”‚
â”‚   (Universal, desde Fase 1)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SWARM::OPS  â”‚     â”‚ XANPAN::AGENTS  â”‚
â”‚ v1.0        â”‚     â”‚ v2.1            â”‚
â”‚ Operaciones â”‚     â”‚ MetodologÃ­a     â”‚
â”‚ (Fase 3-4)  â”‚     â”‚ (Fase 4)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Orden de lectura recomendado:** Chapter 0 â†’ STACK::LLM â†’ Swarm::Ops â†’ Xanpan::Agents. Chapter 0 dice por dÃ³nde empezar. STACK::LLM dice con quÃ© construir. Swarm::Ops dice cÃ³mo operar. Xanpan::Agents dice cÃ³mo gobernar.

**STACK::LLM es universal.** Aplica a los tres perfiles de proyecto. Xanpan::Agents y Swarm::Ops se activan solo cuando el proyecto alcanza el Perfil Completo.

**Mapa de referencias para Perfil Completo:**

| Concepto | STACK::LLM | Xanpan::Agents | Swarm::Ops |
|---|---|---|---|
| Model Router | Â§9.1 (tiers, budget) | Â§9.3 (conceptual) | â€” |
| Evals | Â§9.4 (pipeline) | Â§7.2 (prÃ¡ctica obligatoria) | Â§4.3 (CI insuficiente) |
| Context Engineering | Â§8 (artefactos, economÃ­a) | Â§2.2 (responsabilidad Operador) | â€” |
| Seguridad | Â§6 (OWASP, aislamiento) | Â§13 (gobernanza) | Â§8 (Security-by-Swarm) |
| Observabilidad | Â§5 (3 capas) | Â§12 (dashboard 5D) | Â§7 (agente-observer) |
| CI/CD | Â§4.2 (pipeline) | â€” | Â§4 (sistema nervioso) |
| Deploy | Â§4.3 (feature flags) | Â§10.1 (aprobaciÃ³n humana) | Â§4.2 (flujos concurrentes) |

---

*STACK::LLM v1.0. Febrero 2026.*

*Este stack no es una lista de tecnologÃ­as favoritas. Es una arquitectura diseÃ±ada desde la perspectiva de cÃ³mo los LLMs realmente procesan y generan cÃ³digo. Cada decisiÃ³n prioriza: (1) reducir errores de generaciÃ³n, (2) mantener costos controlados, (3) permitir evoluciÃ³n sin lock-in, y (4) operar de forma segura en producciÃ³n. Aplica igual al inventario de un kiosco que al enjambre mÃ¡s ambicioso. Porque en ambos casos, tu co-desarrollador es un modelo de lenguaje, y merece las mejores condiciones de trabajo.*
