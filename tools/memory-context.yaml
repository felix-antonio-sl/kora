_manifest:
  urn: "urn:kora:doc:memory-context:1.0.0"
  federation:
    visibility: internal
    license: "Proprietary"
  compatibility:
    min_consumer_version: "1.0.0"
    breaking_changes_from: null
  resolution:
    canonical_url: "file://knowledge/domains/moltbot/docs/concepts/kb_memory_context.yaml"
    mirrors: []
  provenance:
    created_by: "Moltbot Docs"
    created_at: "2026-01-29"
    last_modified_by: "Codex"
    last_modified_at: "2026-01-29"
    signature: null

ID: MOLTBOT-KB-CONCEPTS-MEMORY-CONTEXT-01
Version: 1.0.0
Status: Draft
Human-Creator: Moltbot Docs
Human-Editor: Codex
Model-Collaborator: OpenAI-Codex
Creation-Date: '2026-01-29'
Modification-Date: '2026-01-29'
Ctx: 'Moltbot concepts: memory and context'
LLM_Parsing_Instructions:
  ID: KODA-LLM-PARSER-01
  Req: Mandatory block following Metadata.
  Prohib: Using for artifact creation or translation.
  Content: |-
    BEGIN_LLM_INSTRUCTIONS
    You are an AI agent consuming a KODA artifact. Parse with absolute fidelity.

    FIDELITY: Preserve meat (essential information) and skeleton (structure: headers, IDs, lists, tables) with zero loss. Ignore fat (filler words, rhetoric, stylistic prose).

    LEXICON (expand before processing): Act->Action, Cond->Condition, Ctx->Context, Ctx_Required->Required External Reference, Ctx_Optional->Optional External Reference, Def->Definition, Ex->Example, Mssn->Mission, Obj->Objective, Proc->Process, Purp->Purpose, Ref->Reference, XRef->Cross-Artifact Reference, XRef_Required->Mandatory Cross-Artifact Reference, Req->Requirement, Res->Result, Src->Source, Prohib->Prohibition, Warn->Warning, Just->Justification, Rec->Recommendation

    REFERENCE POLICY: Ref: is internal only—must point to existing ID within THIS document. XRef/XRef_Required: are external only—must point to a URN (optionally with #ID fragment) in another artifact. External documents without specific ID use Ctx:, Ctx_Required:, or Ctx_Optional:.

    LANGUAGE POLICY: Keywords in English, content in original language. Never translate content.
    END_LLM_INSTRUCTIONS
Purp: Memory + context
Src:
- File: output/concepts/memory.md
- File: output/concepts/context.md
- File: output/concepts/compaction.md
Sections:
- ID: CONCEPTS-MEMORY
  Src:
    File: output/concepts/memory.md
    URL: https://docs.molt.bot/concepts/memory
  Facts: |-
    H1: Memory - Moltbot
    H1: Memory
    Memory
    Moltbot memory is **plain Markdown in the agent workspace**. The files are the source of truth; the model only "remembers" what gets written to disk. Memory search tools are provided by the active memory plugin (default: `memory-core`). Disable memory plugins with `plugins.slots.memory = "none"`.
    Memory files (Markdown)
    The default workspace layout uses two memory layers:
    - `memory/YYYY-MM-DD.md`
    - Daily log (append-only).
    - Read today + yesterday at session start.
    - `MEMORY.md` (optional)
    - Curated long-term memory.
    - **Only load in the main, private session** (never in group contexts).
    These files live under the workspace (`agents.defaults.workspace`, default `~/clawd`). See [Agent workspace](/concepts/agent-workspace) for the full layout.
    When to write memory
    - Decisions, preferences, and durable facts go to `MEMORY.md`.
    - Day-to-day notes and running context go to `memory/YYYY-MM-DD.md`.
    - If someone says "remember this," write it down (do not keep it in RAM).
    - This area is still evolving. It helps to remind the model to store memories; it will know what to do.
    - If you want something to stick, **ask the bot to write it** into memory.
    Automatic memory flush (pre-compaction ping)
    When a session is **close to auto-compaction** , Moltbot triggers a **silent, agentic turn** that reminds the model to write durable memory **before** the context is compacted. The default prompts explicitly say the model _may reply_ , but usually `NO_REPLY` is the correct response so the user never sees this turn. This is controlled by `agents.defaults.compaction.memoryFlush`:
    Details:
    - **Soft threshold** : flush triggers when the session token estimate crosses `contextWindow - reserveTokensFloor - softThresholdTokens`.
    - **Silent** by default: prompts include `NO_REPLY` so nothing is delivered.
    - **Two prompts** : a user prompt plus a system prompt append the reminder.
    - **One flush per compaction cycle** (tracked in `sessions.json`).
    - **Workspace must be writable** : if the session runs sandboxed with `workspaceAccess: "ro"` or `"none"`, the flush is skipped.
    For the full compaction lifecycle, see [Session management + compaction](/reference/session-management-compaction).
    Vector memory search
    Moltbot can build a small vector index over `MEMORY.md` and `memory/*.md` (plus any extra directories or files you opt in) so semantic queries can find related notes even when wording differs. Defaults:
    - Enabled by default.
    - Watches memory files for changes (debounced).
    - Uses remote embeddings by default. If `memorySearch.provider` is not set, Moltbot auto-selects:
    - `local` if a `memorySearch.local.modelPath` is configured and the file exists.
    - `openai` if an OpenAI key can be resolved.
    - `gemini` if a Gemini key can be resolved.
    - Otherwise memory search stays disabled until configured.
    - Local mode uses node-llama-cpp and may require `pnpm approve-builds`.
    - Uses sqlite-vec (when available) to accelerate vector search inside SQLite.
    Remote embeddings **require** an API key for the embedding provider. Moltbot resolves keys from auth profiles, `models.providers.*.apiKey`, or environment variables. Codex OAuth only covers chat/completions and does **not** satisfy embeddings for memory search. For Gemini, use `GEMINI_API_KEY` or `models.providers.google.apiKey`. When using a custom OpenAI-compatible endpoint, set `memorySearch.remote.apiKey` (and optional `memorySearch.remote.headers`).
    Additional memory paths
    If you want to index Markdown files outside the default workspace layout, add explicit paths:
    Notes:
    - Paths can be absolute or workspace-relative.
    - Directories are scanned recursively for `.md` files.
    - Only Markdown files are indexed.
    - Symlinks are ignored (files or directories).
    Gemini embeddings (native)
    Set the provider to `gemini` to use the Gemini embeddings API directly:
    Notes:
    - `remote.baseUrl` is optional (defaults to the Gemini API base URL).
    - `remote.headers` lets you add extra headers if needed.
    - Default model: `gemini-embedding-001`.
    If you want to use a **custom OpenAI-compatible endpoint** (OpenRouter, vLLM, or a proxy), you can use the `remote` configuration with the OpenAI provider:
    If you don't want to set an API key, use `memorySearch.provider = "local"` or set `memorySearch.fallback = "none"`. Fallbacks:
    - `memorySearch.fallback` can be `openai`, `gemini`, `local`, or `none`.
    - The fallback provider is only used when the primary embedding provider fails.
    Batch indexing (OpenAI + Gemini):
    - Enabled by default for OpenAI and Gemini embeddings. Set `agents.defaults.memorySearch.remote.batch.enabled = false` to disable.
    - Default behavior waits for batch completion; tune `remote.batch.wait`, `remote.batch.pollIntervalMs`, and `remote.batch.timeoutMinutes` if needed.
    - Set `remote.batch.concurrency` to control how many batch jobs we submit in parallel (default: 2).
    - Batch mode applies when `memorySearch.provider = "openai"` or `"gemini"` and uses the corresponding API key.
    - Gemini batch jobs use the async embeddings batch endpoint and require Gemini Batch API availability.
    Why OpenAI batch is fast + cheap:
    - For large backfills, OpenAI is typically the fastest option we support because we can submit many embedding requests in a single batch job and let OpenAI process them asynchronously.
    - OpenAI offers discounted pricing for Batch API workloads, so large indexing runs are usually cheaper than sending the same requests synchronously.
    - See the OpenAI Batch API docs and pricing for details:
    - <https://platform.openai.com/docs/api-reference/batch>
    - <https://platform.openai.com/pricing>
    Config example:
    Tools:
    - `memory_search` - returns snippets with file + line ranges.
    - `memory_get` - read memory file content by path.
    Local mode:
    - Set `agents.defaults.memorySearch.provider = "local"`.
    - Provide `agents.defaults.memorySearch.local.modelPath` (GGUF or `hf:` URI).
    - Optional: set `agents.defaults.memorySearch.fallback = "none"` to avoid remote fallback.
    How the memory tools work
    - `memory_search` semantically searches Markdown chunks (~400 token target, 80-token overlap) from `MEMORY.md` \+ `memory/**/*.md`. It returns snippet text (capped ~700 chars), file path, line range, score, provider/model, and whether we fell back from local -> remote embeddings. No full file payload is returned.
    - `memory_get` reads a specific memory Markdown file (workspace-relative), optionally from a starting line and for N lines. Paths outside `MEMORY.md` / `memory/` are allowed only when explicitly listed in `memorySearch.extraPaths`.
    - Both tools are enabled only when `memorySearch.enabled` resolves true for the agent.
    What gets indexed (and when)
    - File type: Markdown only (`MEMORY.md`, `memory/**/*.md`, plus any `.md` files under `memorySearch.extraPaths`).
    - Index storage: per-agent SQLite at `~/.clawdbot/memory/<agentId>.sqlite` (configurable via `agents.defaults.memorySearch.store.path`, supports `{agentId}` token).
    - Freshness: watcher on `MEMORY.md`, `memory/`, and `memorySearch.extraPaths` marks the index dirty (debounce 1.5s). Sync is scheduled on session start, on search, or on an interval and runs asynchronously. Session transcripts use delta thresholds to trigger background sync.
    - Reindex triggers: the index stores the embedding **provider/model + endpoint fingerprint + chunking params**. If any of those change, Moltbot automatically resets and reindexes the entire store.
    Hybrid search (BM25 + vector)
    When enabled, Moltbot combines:
    - **Vector similarity** (semantic match, wording can differ)
    - **BM25 keyword relevance** (exact tokens like IDs, env vars, code symbols)
    If full-text search is unavailable on your platform, Moltbot falls back to vector-only search.
    Why hybrid?
    Vector search is great at "this means the same thing":
    - "Mac Studio gateway host" vs "the machine running the gateway"
    - "debounce file updates" vs "avoid indexing on every write"
    But it can be weak at exact, high-signal tokens:
    - IDs (`a828e60`, `b3b9895a...`)
    - code symbols (`memorySearch.query.hybrid`)
    - error strings ("sqlite-vec unavailable")
    BM25 (full-text) is the opposite: strong at exact tokens, weaker at paraphrases. Hybrid search is the pragmatic middle ground: **use both retrieval signals** so you get good results for both "natural language" queries and "needle in a haystack" queries.
    How we merge results (the current design)
    Implementation sketch:
    - Retrieve a candidate pool from both sides:
    - **Vector** : top `maxResults * candidateMultiplier` by cosine similarity.
    - **BM25** : top `maxResults * candidateMultiplier` by FTS5 BM25 rank (lower is better).
    - Convert BM25 rank into a 0..1-ish score:
    - `textScore = 1 / (1 + max(0, bm25Rank))`
    - Union candidates by chunk id and compute a weighted score:
    - `finalScore = vectorWeight * vectorScore + textWeight * textScore`
    Notes:
    - `vectorWeight` \+ `textWeight` is normalized to 1.0 in config resolution, so weights behave as percentages.
    - If embeddings are unavailable (or the provider returns a zero-vector), we still run BM25 and return keyword matches.
    - If FTS5 can't be created, we keep vector-only search (no hard failure).
    This isn't "IR-theory perfect", but it's simple, fast, and tends to improve recall/precision on real notes. If we want to get fancier later, common next steps are Reciprocal Rank Fusion (RRF) or score normalization (min/max or z-score) before mixing. Config:
    Embedding cache
    Moltbot can cache **chunk embeddings** in SQLite so reindexing and frequent updates (especially session transcripts) don't re-embed unchanged text. Config:
    Session memory search (experimental)
    You can optionally index **session transcripts** and surface them via `memory_search`. This is gated behind an experimental flag.
    Notes:
    - Session indexing is **opt-in** (off by default).
    - Session updates are debounced and **indexed asynchronously** once they cross delta thresholds (best-effort).
    - `memory_search` never blocks on indexing; results can be slightly stale until background sync finishes.
    - Results still include snippets only; `memory_get` remains limited to memory files.
    - Session indexing is isolated per agent (only that agent's session logs are indexed).
    - Session logs live on disk (`~/.clawdbot/agents/<agentId>/sessions/*.jsonl`). Any process/user with filesystem access can read them, so treat disk access as the trust boundary. For stricter isolation, run agents under separate OS users or hosts.
    Delta thresholds (defaults shown):
    SQLite vector acceleration (sqlite-vec)
    When the sqlite-vec extension is available, Moltbot stores embeddings in a SQLite virtual table (`vec0`) and performs vector distance queries in the database. This keeps search fast without loading every embedding into JS. Configuration (optional):
    Notes:
    - `enabled` defaults to true; when disabled, search falls back to in-process cosine similarity over stored embeddings.
    - If the sqlite-vec extension is missing or fails to load, Moltbot logs the error and continues with the JS fallback (no vector table).
    - `extensionPath` overrides the bundled sqlite-vec path (useful for custom builds or non-standard install locations).
    Local embedding auto-download
    - Default local embedding model: `hf:ggml-org/embeddinggemma-300M-GGUF/embeddinggemma-300M-Q8_0.gguf` (~0.6 GB).
    - When `memorySearch.provider = "local"`, `node-llama-cpp` resolves `modelPath`; if the GGUF is missing it **auto-downloads** to the cache (or `local.modelCacheDir` if set), then loads it. Downloads resume on retry.
    - Native build requirement: run `pnpm approve-builds`, pick `node-llama-cpp`, then `pnpm rebuild node-llama-cpp`.
    - Fallback: if local setup fails and `memorySearch.fallback = "openai"`, we automatically switch to remote embeddings (`openai/text-embedding-3-small` unless overridden) and record the reason.
    Custom OpenAI-compatible endpoint example
    Notes:
    - `remote.*` takes precedence over `models.providers.openai.*`.
    - `remote.headers` merge with OpenAI headers; remote wins on key conflicts. Omit `remote.headers` to use the OpenAI defaults.
  Ex:
  - ID: EX-CONCEPTS-MEMORY-D5251A451C
    Ctx: Copy
    Body: |-
      {
      agents: {
      defaults: {
      compaction: {
      reserveTokensFloor: 20000,
      memoryFlush: {
      enabled: true,
      softThresholdTokens: 4000,
      systemPrompt: "Session nearing compaction. Store durable memories now.",
      prompt: "Write any lasting notes to memory/YYYY-MM-DD.md; reply with NO_REPLY if nothing to store."
      }
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-CCF987769F
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      extraPaths: ["../team-docs", "/srv/shared-notes/overview.md"]
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-D7782A85EE
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      provider: "gemini",
      model: "gemini-embedding-001",
      remote: {
      apiKey: "YOUR_GEMINI_API_KEY"
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-4A06C4059E
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      provider: "openai",
      model: "text-embedding-3-small",
      remote: {
      baseUrl: "https://api.example.com/v1/",
      apiKey: "YOUR_OPENAI_COMPAT_API_KEY",
      headers: { "X-Custom-Header": "value" }
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-90402A8480
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      provider: "openai",
      model: "text-embedding-3-small",
      fallback: "openai",
      remote: {
      batch: { enabled: true, concurrency: 2 }
      },
      sync: { watch: true }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-93BB197C5B
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      query: {
      hybrid: {
      enabled: true,
      vectorWeight: 0.7,
      textWeight: 0.3,
      candidateMultiplier: 4
      }
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-D938C02920
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      cache: {
      enabled: true,
      maxEntries: 50000
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-6231467310
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      experimental: { sessionMemory: true },
      sources: ["memory", "sessions"]
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-2B8CD57AD8
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      sync: {
      sessions: {
      deltaBytes: 100000,   // ~100 KB
      deltaMessages: 50     // JSONL lines
      }
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-09129F8569
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      store: {
      vector: {
      enabled: true,
      extensionPath: "/path/to/sqlite-vec"
      }
      }
      }
      }
      }
  - ID: EX-CONCEPTS-MEMORY-5165DEEBA0
    Ctx: Copy
    Body: |-
      agents: {
      defaults: {
      memorySearch: {
      provider: "openai",
      model: "text-embedding-3-small",
      remote: {
      baseUrl: "https://api.example.com/v1/",
      apiKey: "YOUR_REMOTE_API_KEY",
      headers: {
      "X-Organization": "org-id",
      "X-Project": "project-id"
      }
      }
      }
      }
      }
- ID: CONCEPTS-CONTEXT
  Src:
    File: output/concepts/context.md
    URL: https://docs.molt.bot/concepts/context
  Facts: |-
    H1: Context - Moltbot
    H1: Context
    Context
    "Context" is **everything Moltbot sends to the model for a run**. It is bounded by the model's **context window** (token limit). Beginner mental model:
    - **System prompt** (Moltbot-built): rules, tools, skills list, time/runtime, and injected workspace files.
    - **Conversation history** : your messages + the assistant's messages for this session.
    - **Tool calls/results + attachments** : command output, file reads, images/audio, etc.
    Context is _not the same thing_ as "memory": memory can be stored on disk and reloaded later; context is what's inside the model's current window.
    Quick start (inspect context)
    - `/status` -> quick "how full is my window?" view + session settings.
    - `/context list` -> what's injected + rough sizes (per file + totals).
    - `/context detail` -> deeper breakdown: per-file, per-tool schema sizes, per-skill entry sizes, and system prompt size.
    - `/usage tokens` -> append per-reply usage footer to normal replies.
    - `/compact` -> summarize older history into a compact entry to free window space.
    See also: [Slash commands](/tools/slash-commands), [Token use & costs](/token-use), [Compaction](/concepts/compaction).
    Example output
    Values vary by model, provider, tool policy, and what's in your workspace.
    `/context list`
    `/context detail`
    What counts toward the context window
    Everything the model receives counts, including:
    - System prompt (all sections).
    - Conversation history.
    - Tool calls + tool results.
    - Attachments/transcripts (images/audio/files).
    - Compaction summaries and pruning artifacts.
    - Provider "wrappers" or hidden headers (not visible, still counted).
    How Moltbot builds the system prompt
    The system prompt is **Moltbot-owned** and rebuilt each run. It includes:
    - Tool list + short descriptions.
    - Skills list (metadata only; see below).
    - Workspace location.
    - Time (UTC + converted user time if configured).
    - Runtime metadata (host/OS/model/thinking).
    - Injected workspace bootstrap files under **Project Context**.
    Full breakdown: [System Prompt](/concepts/system-prompt).
    Injected workspace files (Project Context)
    By default, Moltbot injects a fixed set of workspace files (if present):
    - `AGENTS.md`
    - `SOUL.md`
    - `TOOLS.md`
    - `IDENTITY.md`
    - `USER.md`
    - `HEARTBEAT.md`
    - `BOOTSTRAP.md` (first-run only)
    Large files are truncated per-file using `agents.defaults.bootstrapMaxChars` (default `20000` chars). `/context` shows **raw vs injected** sizes and whether truncation happened.
    Skills: what's injected vs loaded on-demand
    The system prompt includes a compact **skills list** (name + description + location). This list has real overhead. Skill instructions are _not_ included by default. The model is expected to `read` the skill's `SKILL.md` **only when needed**.
    Tools: there are two costs
    Tools affect context in two ways:
    - **Tool list text** in the system prompt (what you see as "Tooling").
    - **Tool schemas** (JSON). These are sent to the model so it can call tools. They count toward context even though you don't see them as plain text.
    `/context detail` breaks down the biggest tool schemas so you can see what dominates.
    Commands, directives, and "inline shortcuts"
    Slash commands are handled by the Gateway. There are a few different behaviors:
    - **Standalone commands** : a message that is only `/...` runs as a command.
    - **Directives** : `/think`, `/verbose`, `/reasoning`, `/elevated`, `/model`, `/queue` are stripped before the model sees the message.
    - Directive-only messages persist session settings.
    - Inline directives in a normal message act as per-message hints.
    - **Inline shortcuts** (allowlisted senders only): certain `/...` tokens inside a normal message can run immediately (example: "hey /status"), and are stripped before the model sees the remaining text.
    Details: [Slash commands](/tools/slash-commands).
    Sessions, compaction, and pruning (what persists)
    What persists across messages depends on the mechanism:
    - **Normal history** persists in the session transcript until compacted/pruned by policy.
    - **Compaction** persists a summary into the transcript and keeps recent messages intact.
    - **Pruning** removes old tool results from the _in-memory_ prompt for a run, but does not rewrite the transcript.
    Docs: [Session](/concepts/session), [Compaction](/concepts/compaction), [Session pruning](/concepts/session-pruning).
    What `/context` actually reports
    `/context` prefers the latest **run-built** system prompt report when available:
    - `System prompt (run)` = captured from the last embedded (tool-capable) run and persisted in the session store.
    - `System prompt (estimate)` = computed on the fly when no run report exists (or when running via a CLI backend that doesn't generate the report).
    Either way, it reports sizes and top contributors; it does **not** dump the full system prompt or tool schemas.
  Ex:
  - ID: EX-CONCEPTS-CONTEXT-4A3C4B0075
    Ctx: Copy
    Body: |2-
       Context breakdown
      Workspace: <workspaceDir>
      Bootstrap max/file: 20,000 chars
      Sandbox: mode=non-main sandboxed=false
      System prompt (run): 38,412 chars (~9,603 tok) (Project Context 23,901 chars (~5,976 tok))
  - ID: EX-CONCEPTS-CONTEXT-17B91FBD4C
    Body: |-
      Injected workspace files:
      - AGENTS.md: OK | raw 1,742 chars (~436 tok) | injected 1,742 chars (~436 tok)
      - SOUL.md: OK | raw 912 chars (~228 tok) | injected 912 chars (~228 tok)
      - TOOLS.md: TRUNCATED | raw 54,210 chars (~13,553 tok) | injected 20,962 chars (~5,241 tok)
      - IDENTITY.md: OK | raw 211 chars (~53 tok) | injected 211 chars (~53 tok)
      - USER.md: OK | raw 388 chars (~97 tok) | injected 388 chars (~97 tok)
      - HEARTBEAT.md: MISSING | raw 0 | injected 0
      - BOOTSTRAP.md: OK | raw 0 chars (~0 tok) | injected 0 chars (~0 tok)
  - ID: EX-CONCEPTS-CONTEXT-825550997D
    Body: |-
      Skills list (system prompt text): 2,184 chars (~546 tok) (12 skills)
      Tools: read, edit, write, exec, process, browser, message, sessions_send, ...
      Tool list (system prompt text): 1,032 chars (~258 tok)
      Tool schemas (JSON): 31,988 chars (~7,997 tok) (counts toward context; not shown as text)
      Tools: (same as above)
  - ID: EX-CONCEPTS-CONTEXT-D5526D963C
    Body: 'Session tokens (cached): 14,250 total / ctx=32,000'
  - ID: EX-CONCEPTS-CONTEXT-3311ACD357
    Ctx: Copy
    Body: |2-
       Context breakdown (detailed)
      ...
      Top skills (prompt entry size):
      - frontend-design: 412 chars (~103 tok)
      - oracle: 401 chars (~101 tok)
      ... (+10 more skills)
  - ID: EX-CONCEPTS-CONTEXT-6C336AA0CC
    Body: |-
      Top tools (schema size):
      - browser: 9,812 chars (~2,453 tok)
      - exec: 6,240 chars (~1,560 tok)
      ... (+N more tools)
- ID: CONCEPTS-COMPACTION
  Src:
    File: output/concepts/compaction.md
    URL: https://docs.molt.bot/concepts/compaction
  Facts: |-
    H1: Compaction - Moltbot
    H1: Compaction
    Context Window & Compaction
    Every model has a **context window** (max tokens it can see). Long-running chats accumulate messages and tool results; once the window is tight, Moltbot **compacts** older history to stay within limits.
    What compaction is
    Compaction **summarizes older conversation** into a compact summary entry and keeps recent messages intact. The summary is stored in the session history, so future requests use:
    - The compaction summary
    - Recent messages after the compaction point
    Compaction **persists** in the session's JSONL history.
    Configuration
    See [Compaction config & modes](/concepts/compaction) for the `agents.defaults.compaction` settings.
    Auto-compaction (default on)
    When a session nears or exceeds the model's context window, Moltbot triggers auto-compaction and may retry the original request using the compacted context. You'll see:
    - ` Auto-compaction complete` in verbose mode
    - `/status` showing ` Compactions: <count>`
    Before compaction, Moltbot can run a **silent memory flush** turn to store durable notes to disk. See [Memory](/concepts/memory) for details and config.
    Manual compaction
    Use `/compact` (optionally with instructions) to force a compaction pass:
    Context window source
    Context window is model-specific. Moltbot uses the model definition from the configured provider catalog to determine limits.
    Compaction vs pruning
    - **Compaction** : summarises and **persists** in JSONL.
    - **Session pruning** : trims old **tool results** only, **in-memory** , per request.
    See [/concepts/session-pruning](/concepts/session-pruning) for pruning details.
    Tips
    - Use `/compact` when sessions feel stale or context is bloated.
    - Large tool outputs are already truncated; pruning can further reduce tool-result buildup.
    - If you need a fresh slate, `/new` or `/reset` starts a new session id.
  Ex:
  - ID: EX-CONCEPTS-COMPACTION-03FE008D87
    Ctx: Copy
    Body: /compact Focus on decisions and open questions
